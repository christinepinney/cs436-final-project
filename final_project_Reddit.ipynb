{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f69b4d",
   "metadata": {},
   "source": [
    "# Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d7e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opendatasets\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import opendatasets as od\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel, ElectraModel, ElectraTokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from torch import cuda\n",
    "from tqdm.notebook import tqdm\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "from collections import Counter as ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530f06c",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45acb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#od.download(\"https://www.kaggle.com/datasets/danofer/sarcasm?resource=download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3217b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010821</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "      <td>TwarkMain</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-04</td>\n",
       "      <td>2009-04-25 00:47:52</td>\n",
       "      <td>No one is calling this an engineered pathogen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010822</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "      <td>BCHarvey</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-05</td>\n",
       "      <td>2009-05-14 22:27:40</td>\n",
       "      <td>In a move typical of their recent do-nothing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010823</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "      <td>rebelcommander</td>\n",
       "      <td>atheism</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-11 00:22:57</td>\n",
       "      <td>Screw the Disabled--I've got to get to Church ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010824</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "      <td>catsi</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-23 21:12:49</td>\n",
       "      <td>I've always been unsettled by that. I hear a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010825</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "      <td>frogking</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-24 06:20:14</td>\n",
       "      <td>Why do the people who make our laws seem unabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010826 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment  \\\n",
       "0            0                                         NC and NH.   \n",
       "1            0  You do know west teams play against west teams...   \n",
       "2            0  They were underdogs earlier today, but since G...   \n",
       "3            0  This meme isn't funny none of the \"new york ni...   \n",
       "4            0                    I could use one of those tools.   \n",
       "...        ...                                                ...   \n",
       "1010821      1  I'm sure that Iran and N. Korea have the techn...   \n",
       "1010822      1                 whatever you do, don't vote green!   \n",
       "1010823      1  Perhaps this is an atheist conspiracy to make ...   \n",
       "1010824      1  The Slavs got their own country - it is called...   \n",
       "1010825      1  values, as in capitalism .. there is good mone...   \n",
       "\n",
       "                 author           subreddit  score  ups  downs     date  \\\n",
       "0             Trumpbart            politics      2   -1     -1  2016-10   \n",
       "1             Shbshb906                 nba     -4   -1     -1  2016-11   \n",
       "2              Creepeth                 nfl      3    3      0  2016-09   \n",
       "3             icebrotha  BlackPeopleTwitter     -8   -1     -1  2016-10   \n",
       "4             cush2push  MaddenUltimateTeam      6   -1     -1  2016-12   \n",
       "...                 ...                 ...    ...  ...    ...      ...   \n",
       "1010821       TwarkMain          reddit.com      2    2      0  2009-04   \n",
       "1010822        BCHarvey             climate      1    1      0  2009-05   \n",
       "1010823  rebelcommander             atheism      1    1      0  2009-01   \n",
       "1010824           catsi           worldnews      1    1      0  2009-01   \n",
       "1010825        frogking            politics      2    2      0  2009-01   \n",
       "\n",
       "                 created_utc  \\\n",
       "0        2016-10-16 23:55:23   \n",
       "1        2016-11-01 00:24:10   \n",
       "2        2016-09-22 21:45:37   \n",
       "3        2016-10-18 21:03:47   \n",
       "4        2016-12-30 17:00:13   \n",
       "...                      ...   \n",
       "1010821  2009-04-25 00:47:52   \n",
       "1010822  2009-05-14 22:27:40   \n",
       "1010823  2009-01-11 00:22:57   \n",
       "1010824  2009-01-23 21:12:49   \n",
       "1010825  2009-01-24 06:20:14   \n",
       "\n",
       "                                            parent_comment  \n",
       "0        Yeah, I get that argument. At this point, I'd ...  \n",
       "1        The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                                  They're favored to win.  \n",
       "3                               deadass don't kill my buzz  \n",
       "4        Yep can confirm I saw the tool they use for th...  \n",
       "...                                                    ...  \n",
       "1010821  No one is calling this an engineered pathogen,...  \n",
       "1010822  In a move typical of their recent do-nothing a...  \n",
       "1010823  Screw the Disabled--I've got to get to Church ...  \n",
       "1010824  I've always been unsettled by that. I hear a l...  \n",
       "1010825  Why do the people who make our laws seem unabl...  \n",
       "\n",
       "[1010826 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('sarcasm/train-balanced-sarcasm.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ace47218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 505413, 1: 505413})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e4cab",
   "metadata": {},
   "source": [
    "##### drop data with low scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f262128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['score'] > 10]\n",
    "data = data.drop(\"score\", axis=1)\n",
    "\n",
    "data = data[30000:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a288b0",
   "metadata": {},
   "source": [
    "##### analyze lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cfdd002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224, 1, 60.53053333333333)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data['comment'].str.len()), min(data['comment'].str.len()), data['comment'].str.len().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483094b",
   "metadata": {},
   "source": [
    "##### normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80485313",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment'] = data['comment'].astype(str)\n",
    "data['comment'] = data['comment'].apply(lambda x: x.lower())\n",
    "\n",
    "# def remove_punctuation(s):\n",
    "#     s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "#     return s\n",
    "# data['comment'] = data['comment'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59657e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemma.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "data['comment'] = data['comment'].apply(lemmatize_text)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1fec8",
   "metadata": {},
   "source": [
    "## Train/Test Split for MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8919136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christinepinney/miniconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('glasgow_stop_words.txt') as f:\n",
    "    stops = f.readlines()\n",
    "f.close()\n",
    "vec = CountVectorizer(stop_words=stops)\n",
    "bag_o_words = vec.fit_transform(data['comment'])\n",
    "bag_o_words = np.array(bag_o_words.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5f7b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_o_words\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5149398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa98c666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6335353535353535\n",
      "F1 score: 0.6276945530535158\n",
      "ROC AUC: 0.6286198807584101\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('F1 score:', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d50bb634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.54      0.58      4684\n",
      "           1       0.63      0.72      0.67      5216\n",
      "\n",
      "    accuracy                           0.63      9900\n",
      "   macro avg       0.63      0.63      0.63      9900\n",
      "weighted avg       0.63      0.63      0.63      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cd85023",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data={'predicted': y_pred, 'actual': y_test})\n",
    "predictions = results.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9256240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(predicted, actual):\n",
    "    if predicted == actual:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "predictions['correct'] = predictions.apply(lambda x: is_correct(x.predicted, x.actual), axis=1)\n",
    "predictions = predictions[['comment','predicted','actual','correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a84e48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec8f47",
   "metadata": {},
   "source": [
    "##### looking at where the model made correct/incorrect decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46241fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491152</th>\n",
       "      <td>those penalty shootouts...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426279</th>\n",
       "      <td>mossad obviously used it mind control to make ken livingstone put his foot in his mouth.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352881</th>\n",
       "      <td>also the hangar wa getting pretty filled up and they didn't want to have to deal with yet another core taking up space.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275010</th>\n",
       "      <td>ah - thanks for the laugh, anonymous teenager.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270178</th>\n",
       "      <td>it got the stats of a dry rot so it can be good</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355119</th>\n",
       "      <td>kai should be a professional dancer, not a bodybuilder.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322020</th>\n",
       "      <td>still great at crushing highschool pus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285332</th>\n",
       "      <td>for all the complaint that saudi arabia is exporting it radical form on islam (wahhabism), american church are the one funding the rise of fundamental christianity in most of africa.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316065</th>\n",
       "      <td>reverse psychology not working, still going to non cannon bloodvelds in nieves cave</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461111</th>\n",
       "      <td>rip 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                       comment  \\\n",
       "491152                                                                                                                                                              those penalty shootouts...   \n",
       "426279                                                                                                mossad obviously used it mind control to make ken livingstone put his foot in his mouth.   \n",
       "352881                                                                 also the hangar wa getting pretty filled up and they didn't want to have to deal with yet another core taking up space.   \n",
       "275010                                                                                                                                          ah - thanks for the laugh, anonymous teenager.   \n",
       "270178                                                                                                                                         it got the stats of a dry rot so it can be good   \n",
       "355119                                                                                                                                 kai should be a professional dancer, not a bodybuilder.   \n",
       "322020                                                                                                                                                  still great at crushing highschool pus   \n",
       "285332  for all the complaint that saudi arabia is exporting it radical form on islam (wahhabism), american church are the one funding the rise of fundamental christianity in most of africa.   \n",
       "316065                                                                                                     reverse psychology not working, still going to non cannon bloodvelds in nieves cave   \n",
       "461111                                                                                                                                                                                rip 2016   \n",
       "\n",
       "        predicted  actual  correct  \n",
       "491152          1       0    False  \n",
       "426279          0       1    False  \n",
       "352881          0       1    False  \n",
       "275010          1       0    False  \n",
       "270178          1       0    False  \n",
       "355119          1       0    False  \n",
       "322020          0       1    False  \n",
       "285332          1       0    False  \n",
       "316065          0       1    False  \n",
       "461111          0       1    False  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[predictions['correct']==False].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e80f0db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323164</th>\n",
       "      <td>also: utility bills, dental visits, car maintenance...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501932</th>\n",
       "      <td>you forgot the</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309385</th>\n",
       "      <td>your comment is curious, because it seems you either don't believe that *your* child's behavior is the result of how you've raised him or you're assuming the zoo child doesn't have a \"disorder\".</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363936</th>\n",
       "      <td>create au azeroth for 1-60, problem solved.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348978</th>\n",
       "      <td>did you just assume that person gender?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330485</th>\n",
       "      <td>well, good thing you aren't an island and there isn't a single gay person in the uk.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286374</th>\n",
       "      <td>we had an aunt that would kiss u and then grab our packages.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385884</th>\n",
       "      <td>but don't you realise that pot use eventually lead to heroin use?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294095</th>\n",
       "      <td>fuck, i wish they would.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256861</th>\n",
       "      <td>because it's all decoration and ha absolutely no other negative affect on the people who live/work here</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                   comment  \\\n",
       "323164                                                                                                                                              also: utility bills, dental visits, car maintenance...   \n",
       "501932                                                                                                                                                                                      you forgot the   \n",
       "309385  your comment is curious, because it seems you either don't believe that *your* child's behavior is the result of how you've raised him or you're assuming the zoo child doesn't have a \"disorder\".   \n",
       "363936                                                                                                                                                         create au azeroth for 1-60, problem solved.   \n",
       "348978                                                                                                                                                             did you just assume that person gender?   \n",
       "330485                                                                                                                well, good thing you aren't an island and there isn't a single gay person in the uk.   \n",
       "286374                                                                                                                                        we had an aunt that would kiss u and then grab our packages.   \n",
       "385884                                                                                                                                   but don't you realise that pot use eventually lead to heroin use?   \n",
       "294095                                                                                                                                                                            fuck, i wish they would.   \n",
       "256861                                                                                             because it's all decoration and ha absolutely no other negative affect on the people who live/work here   \n",
       "\n",
       "        predicted  actual  correct  \n",
       "323164          0       0     True  \n",
       "501932          1       1     True  \n",
       "309385          0       0     True  \n",
       "363936          1       1     True  \n",
       "348978          1       1     True  \n",
       "330485          1       1     True  \n",
       "286374          0       0     True  \n",
       "385884          1       1     True  \n",
       "294095          0       0     True  \n",
       "256861          1       1     True  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[predictions['correct']==True].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfdf42",
   "metadata": {},
   "source": [
    "## Train/Test Split for ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4ea0a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(data, test_size = 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c4d735a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data[['comment', 'label']]\n",
    "testing_data = testing_data[['comment', 'label']]\n",
    "\n",
    "# training_data = training_data[['parent_comment', 'label']]\n",
    "# testing_data = testing_data[['parent_comment', 'label']]\n",
    "# training_data = training_data[['text', 'label']]\n",
    "# testing_data = testing_data[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "73b96493",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.get_dummies(training_data.label)\n",
    "test_y = pd.get_dummies(testing_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b807e7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27600,), (27600, 2), (2400,), (2400, 2))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array(training_data['comment'])\n",
    "test_data = np.array(testing_data['comment'])\n",
    "\n",
    "# train_data = np.array(training_data['parent_comment'])\n",
    "# test_data = np.array(testing_data['parent_comment'])\n",
    "# train_data = np.array(training_data['text'])\n",
    "# test_data = np.array(testing_data['text'])\n",
    "\n",
    "train_labels = LabelBinarizer().fit_transform(train_y)\n",
    "test_labels = LabelBinarizer().fit_transform(test_y)\n",
    "\n",
    "train_data.shape, train_labels.shape, test_data.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "566e73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:500]\n",
    "train_labels = train_labels[:500]\n",
    "\n",
    "test_data = test_data[:100]\n",
    "test_labels = test_labels[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce2a2e1",
   "metadata": {},
   "source": [
    "## Tokenize Data & Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "964abfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "906af8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text\n",
    "        self.targets = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': self.targets[index].clone().detach()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "afc4288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAClass(torch.nn.Module):\n",
    "    def __init__(self, NUM_OUT):\n",
    "        super(ELECTRAClass, self).__init__()\n",
    "                   \n",
    "        self.l1 = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n",
    "        self.classifier = torch.nn.Linear(256, NUM_OUT)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        classifier = self.classifier(pooler)\n",
    "        dropout = self.dropout(classifier)\n",
    "        output = self.softmax(dropout)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "504720b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCELoss()(outputs, targets)\n",
    "\n",
    "def train(model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    for data in tqdm(training_loader):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss\n",
    "    \n",
    "def validation(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testing_loader):\n",
    "            targets = data['targets']\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach()\n",
    "            fin_outputs.extend(outputs)\n",
    "            fin_targets.extend(targets)\n",
    "    return torch.stack(fin_outputs), torch.stack(fin_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6755b41",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5aea5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 4\n",
    "NUM_OUT = 2\n",
    "LEARNING_RATE = 2e-05\n",
    "\n",
    "training_data = MultiLabelDataset(train_data, torch.from_numpy(train_labels), tokenizer, MAX_LEN)\n",
    "testing_data = MultiLabelDataset(test_data, torch.from_numpy(test_labels), tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }    \n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, **train_params)\n",
    "testing_loader = torch.utils.data.DataLoader(testing_data, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "567cd8cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf75fba354f3429193cc262fc204eeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7720845937728882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a95077d59e4f05a4ee298295bf5c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set 0.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7e9ff874a2433fa26708293526f7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.621605396270752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5d1b47bd2b4345b472961718ccff97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set 0.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5296e2ec3a841f4ae9fa34ba96f0d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss:  0.8914043307304382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f998c61d074b7c94505320efc0a052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745f3e12d2e0424296df68762aeb4557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss:  0.6579529047012329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36856a46e044c8ab6131c487c6ad90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set 0.52\n"
     ]
    }
   ],
   "source": [
    "model = ELECTRAClass(NUM_OUT)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(model, training_loader, optimizer)\n",
    "    print(f'Epoch: {epoch}, Loss:  {loss.item()}')  \n",
    "    guess, targs = validation(model, testing_loader)\n",
    "    guesses = torch.max(guess, dim=1)\n",
    "    targets = torch.max(targs, dim=1)\n",
    "    print('accuracy on test set {}'.format(accuracy_score(guesses.indices, targets.indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f160af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4fa513a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
